# ğŸ˜ JUMBOBOT â€“ Tracker de Precios Jumbo Argentina

Seguimiento diario automatizado de precios del supermercado **Jumbo** (jumbo.com.ar).

## Â¿QuÃ© hace?

- ğŸ• **Scraper diario** automÃ¡tico vÃ­a GitHub Actions (9 AM Argentina)
- ğŸ“¦ ~383 categorÃ­as de Jumbo vÃ­a API VTEX Intelligent Search
- âš¡ Scraping paralelo (~5 minutos en total con 8 workers)
- ğŸ“Š Web con grÃ¡ficos histÃ³ricos, rankings y variaciones
- ğŸ¦ Tweet resumen diario en X (opcional)

## CÃ³mo funciona tÃ©cnicamente

Jumbo usa **VTEX IO** con el endpoint de **Intelligent Search**:

```
GET https://www.jumbo.com.ar/api/io/_v/api/intelligent-search/product_search/category-3/{slug}
    ?from=0&to=49&sort=price:desc
```

El scraper obtiene el Ã¡rbol de categorÃ­as dinÃ¡micamente:
```
GET https://www.jumbo.com.ar/api/catalog_system/pub/category/tree/3
```

Y recorre los 3 niveles: nivel1 (cat_principal) â†’ nivel2 (cat_padre) â†’ nivel3 (slug).

## Estructura del proyecto

```
JUMBOBOT/
â”œâ”€â”€ jumbo_scraper.py              â† Scraper paralelo (8 workers)
â”œâ”€â”€ analizar_precios_jumbo.py     â† Genera JSONs de historial y rankings
â”œâ”€â”€ generar_web_jumbo.py          â† Genera docs/index.html (GitHub Pages)
â”œâ”€â”€ tweetear_jumbo.py             â† Publica resumen diario en X/Twitter
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/                         â† JSONs generados (histÃ³rico, grÃ¡ficos, rankings)
â”œâ”€â”€ docs/                         â† Sitio web estÃ¡tico (GitHub Pages)
â”œâ”€â”€ output_jumbo/                 â† CSVs crudos del scraper (gitignore)
â””â”€â”€ .github/workflows/
    â”œâ”€â”€ scraper_diario.yml        â† Cron diario 9 AM Argentina
    â””â”€â”€ regenerar_web.yml         â† Trigger manual para regenerar la web
```

## Setup (5 minutos)

### 1. Subir el repo a GitHub

### 2. Agregar output_jumbo/ al .gitignore
Crear archivo `.gitignore` con:
```
output_jumbo/
__pycache__/
*.pyc
```

### 3. Habilitar GitHub Pages
- Settings â†’ Pages â†’ Branch: `main`, carpeta: `/docs`
- Tu web quedarÃ¡ en `https://TU_USUARIO.github.io/NOMBRE_REPO`

### 4. Primer scraping manual
- Actions â†’ "Scraper Diario Jumbo" â†’ **Run workflow**
- Tarda ~5 minutos (con scraper paralelo)

### 5. (Opcional) Tweets automÃ¡ticos
Agregar en Settings â†’ Secrets â†’ Actions:
- `X_API_KEY`
- `X_API_SECRET`
- `X_ACCESS_TOKEN`
- `X_ACCESS_SECRET`

## CategorÃ­as

El scraper descarga el Ã¡rbol de categorÃ­as automÃ¡ticamente desde Jumbo.
Las categorÃ­as de nivel 1 (cat_principal) que usa el anÃ¡lisis son:

- AlmacÃ©n
- Bebidas Con Alcohol
- Bebidas Sin Alcohol
- Frescos
- Congelados
- Limpieza
- Cuidado Personal

Si Jumbo agrega o renombra categorÃ­as, el scraper las captura automÃ¡ticamente.
Solo actualizar `ORDEN_CATS` en `analizar_precios_jumbo.py` si cambian los nombres de nivel 1.

## Ajuste de workers

En `jumbo_scraper.py`:
```python
WORKERS = 8   # categorÃ­as en paralelo â€” bajar a 4-5 si hay muchos errores 429
```

## Licencia

MIT â€“ Uso educativo / transparencia de precios. No afiliado con Cencosud/Jumbo.
